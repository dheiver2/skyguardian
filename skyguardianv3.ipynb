{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8377042,"sourceType":"datasetVersion","datasetId":4974923},{"sourceId":8380181,"sourceType":"datasetVersion","datasetId":4983338},{"sourceId":138401128,"sourceType":"kernelVersion"},{"sourceId":46340,"sourceType":"modelInstanceVersion","modelInstanceId":38840},{"sourceId":45698,"sourceType":"modelInstanceVersion","modelInstanceId":38313}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\nimport numpy as np\nimport urllib\n\ndef open_input(input_path):\n    \"\"\"\n    Abre a entrada com base no tipo (imagem, vídeo ou streaming) e retorna\n    o objeto VideoCapture correspondente.\n    \"\"\"\n    if input_path.startswith('http'):  # Se for um link de streaming\n        stream = urllib.request.urlopen(input_path)\n        bytes = bytearray()\n        while True:\n            bytes += stream.read(1024)\n            a = bytes.find(b'\\xff\\xd8')\n            b = bytes.find(b'\\xff\\xd9')\n            if a != -1 and b != -1:\n                jpg = bytes[a:b + 2]\n                bytes = bytes[b + 2:]\n                frame = cv2.imdecode(np.frombuffer(jpg, dtype=np.uint8), cv2.IMREAD_COLOR)\n                return cv2.VideoCapture(frame)\n    elif input_path.endswith(('jpg', 'jpeg', 'png', 'bmp')):  # Se for uma imagem\n        return cv2.VideoCapture(input_path)\n    else:  # Se for um vídeo\n        return cv2.VideoCapture(input_path)\n\ndef process_input(input_path):\n    \"\"\"\n    Processa a entrada com base no tipo (imagem, vídeo ou streaming) e retorna\n    o objeto VideoCapture correspondente e a largura e altura dos quadros.\n    \"\"\"\n    cap = open_input(input_path)\n\n    # Obtém a largura e a altura do quadro do vídeo\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    return cap, frame_width, frame_height\n\ndef process_frame(frame, input_type):\n    \"\"\"\n    Processa um único quadro do vídeo.\n    Retorna o quadro com as previsões desenhadas\n    e uma lista de todas as previsões.\n    \"\"\"\n    # Faz previsões no quadro\n    predictions = model.predict(frame, conf=confidence_threshold)\n\n    # Extrai informações das previsões apenas para a classe de pessoa\n    frame_predictions = []\n    for prediction in predictions:\n        class_names = prediction.class_names\n        labels = prediction.prediction.labels\n        confidence = prediction.prediction.confidence\n        bboxes = prediction.prediction.bboxes_xyxy\n\n        for label, conf, bbox in zip(labels, confidence, bboxes):\n            # Verifica se o rótulo é para uma pessoa\n            if class_names[int(label)] == 'person' and conf >= confidence_threshold:\n                # Salva a previsão no formato desejado\n                frame_predictions.append({\n                    \"class_name\": class_names[int(label)],\n                    \"confidence\": conf,\n                    \"bbox\": bbox\n                })\n\n                # Desenha a caixa delimitadora e o rótulo no quadro\n                xmin, ymin, xmax, ymax = map(int, bbox)\n                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n                cv2.putText(frame, f'Person: {conf:.2f}', (xmin, ymin - 10),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n    \n    return frame, frame_predictions\n\n# Define o caminho da entrada (imagem, vídeo ou link de streaming)\nINPUT_PATH = \"/kaggle/input/video/videoplayback (1).mp4\"\n\n# Processa a entrada com base no tipo\ncap, frame_width, frame_height = process_input(INPUT_PATH)\n\n# Define o codec e cria o objeto VideoWriter\noutput_path = \"/kaggle/working/output_video.mp4\"\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))\n\n# Carrega o modelo YOLO-NAS-L\nmodel = models.get(Models.YOLO_NAS_L, pretrained_weights=\"coco\")\n\n# Define o limite de confiança inicial\nconfidence_threshold = 0.2\n\n# Cria um arquivo de log para salvar as previsões\nlog_file = open(\"/kaggle/working/predictions_log.txt\", \"w\")\n\n# Processa cada frame do vídeo\nframe_count = 0\nwhile cap.isOpened() and frame_count < 1000:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Processa o quadro e obtém as previsões\n    frame_processed, frame_predictions = process_frame(frame, input_type='video')\n\n    # Escreve o quadro processado no vídeo de saída\n    out.write(frame_processed)\n\n    # Salva as previsões no arquivo de log\n    for prediction in frame_predictions:\n        log_file.write(f\"Class: {prediction['class_name']}, Confidence: {prediction['confidence']}, BBox: {prediction['bbox']}\\n\")\n\n    frame_count += 1\n\n# Fecha o arquivo de log\nlog_file.close()\n\n# Libera os objetos de captura de vídeo e gravação\ncap.release()\nout.release()\n\nprint(\"Vídeo com previsões salvo com sucesso em:\", output_path)\nprint(\"Arquivo de log com as previsões salvo com sucesso.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-11T13:37:52.282719Z","iopub.execute_input":"2024-05-11T13:37:52.283146Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"[2024-05-11 13:37:53] INFO - checkpoint_utils.py - License Notification: YOLO-NAS pre-trained weights are subjected to the specific license terms and conditions detailed in \nhttps://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS.md\nBy downloading the pre-trained weight files you agree to comply with these terms.\n[2024-05-11 13:37:53] INFO - checkpoint_utils.py - Successfully loaded pretrained weights for architecture yolo_nas_l\n[2024-05-11 13:37:53] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:37:54] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:37:55] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:37:56] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:37:57] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:37:58] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:37:59] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:38:00] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:38:01] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:38:02] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:38:03] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:38:04] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:38:05] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:38:06] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:38:07] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 13:38:08] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n","output_type":"stream"}]}]}