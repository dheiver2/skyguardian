{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8377042,"sourceType":"datasetVersion","datasetId":4974923},{"sourceId":8380181,"sourceType":"datasetVersion","datasetId":4983338},{"sourceId":138401128,"sourceType":"kernelVersion"},{"sourceId":46340,"sourceType":"modelInstanceVersion","modelInstanceId":38840},{"sourceId":45698,"sourceType":"modelInstanceVersion","modelInstanceId":38313}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport urllib\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import models\n\nclass ObjectDetector:\n    def __init__(self, model):\n        self.model = model\n\n    def process_frame(self, frame, confidence_threshold):\n        predictions = self.model.predict(frame, conf=confidence_threshold)\n        frame_predictions = []\n        for prediction in predictions:\n            class_names = prediction.class_names\n            labels = prediction.prediction.labels\n            confidence = prediction.prediction.confidence\n            bboxes = prediction.prediction.bboxes_xyxy\n\n            for label, conf, bbox in zip(labels, confidence, bboxes):\n                if class_names[int(label)] == 'person' and conf >= confidence_threshold:\n                    frame_predictions.append({\n                        \"class_name\": class_names[int(label)],\n                        \"confidence\": conf,\n                        \"bbox\": bbox\n                    })\n\n                    xmin, ymin, xmax, ymax = map(int, bbox)\n                    cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n                    cv2.putText(frame, f'Person: {conf:.2f}', (xmin, ymin - 10),\n                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n        return frame, frame_predictions\n\n    def open_input(self, input_path):\n        if input_path.startswith('http'): \n            stream = urllib.request.urlopen(input_path)\n            bytes = bytearray()\n            while True:\n                bytes += stream.read(1024)\n                a = bytes.find(b'\\xff\\xd8')\n                b = bytes.find(b'\\xff\\xd9')\n                if a != -1 and b != -1:\n                    jpg = bytes[a:b + 2]\n                    bytes = bytes[b + 2:]\n                    frame = cv2.imdecode(np.frombuffer(jpg, dtype=np.uint8), cv2.IMREAD_COLOR)\n                    return cv2.VideoCapture(frame)\n        elif input_path.endswith(('jpg', 'jpeg', 'png', 'bmp')): \n            return cv2.VideoCapture(input_path)\n        else:  \n            return cv2.VideoCapture(input_path)\n\n    def process_input(self, input_path):\n        cap = self.open_input(input_path)\n        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        return cap, frame_width, frame_height\n\n    def detect_objects(self, input_path, output_path, max_frames=None, confidence_threshold=0.2):\n        cap, frame_width, frame_height = self.process_input(input_path)\n\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))\n\n        frame_count = 0\n        while cap.isOpened() and (max_frames is None or frame_count < max_frames):\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            frame_processed, _ = self.process_frame(frame, confidence_threshold)\n\n            out.write(frame_processed)\n            frame_count += 1\n\n        cap.release()\n        out.release()\n        print(\"Vídeo com previsões salvo com sucesso em:\", output_path)\n\n\n# Carregue o modelo YOLO-NAS-L\nmodel = models.get(Models.YOLO_NAS_L, pretrained_weights=\"coco\")\n\n# Instancie o ObjectDetector com o modelo carregado\ndetector = ObjectDetector(model)\n\n# Caminho do vídeo de entrada\ninput_video_path = \"/kaggle/input/video/videoplayback (1).mp4\"\n\n# Caminho do vídeo de saída com as previsões\noutput_video_path = \"/kaggle/working/saida_com_previsoes.mp4\"\n\n# Número máximo de quadros a serem processados (opcional)\nmax_frames = 500  # Processará apenas os primeiros 500 quadros\n\n# Limiar de confiança para detecção de objetos (opcional)\nconfidence_threshold = 0.2\n\n# Chame o método detect_objects para processar o vídeo e salvar o vídeo de saída com previsões\ndetector.detect_objects(input_video_path, output_video_path, max_frames=max_frames, confidence_threshold=confidence_threshold)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-11T14:24:13.137764Z","iopub.execute_input":"2024-05-11T14:24:13.138148Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"[2024-05-11 14:24:13] INFO - checkpoint_utils.py - License Notification: YOLO-NAS pre-trained weights are subjected to the specific license terms and conditions detailed in \nhttps://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS.md\nBy downloading the pre-trained weight files you agree to comply with these terms.\n[2024-05-11 14:24:14] INFO - checkpoint_utils.py - Successfully loaded pretrained weights for architecture yolo_nas_l\n[2024-05-11 14:24:14] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 14:24:15] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 14:24:16] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 14:24:17] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 14:24:18] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 14:24:19] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 14:24:20] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 14:24:21] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n[2024-05-11 14:24:22] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n","output_type":"stream"}]}]}